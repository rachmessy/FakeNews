{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28eb37c1-5f14-43b5-a219-f18fa0a4013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LIBRARY IMPORTS ----\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfa0bc2e-c2d1-430b-921f-d96029753128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---- RETRIEVE DATA ----\n",
    "Source:  https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\n",
    "\"\"\"\n",
    "# Create column names\n",
    "column_names = ['id', 'label', 'statement', 'subjects', 'speaker', 'job_title', 'state', 'party_affiliation', 'credit_history_count',\\\n",
    "         'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count', 'context']\n",
    "\n",
    "# specify data types\n",
    "dtypes = {0: 'string', 1: 'string', 2: 'string', 3: 'string', 4: 'string', 5: 'string', 6: 'string', 7: 'string',\\\n",
    "          8: 'UInt64', 9: 'UInt64', 10: 'UInt64', 11: 'UInt64', 12: 'UInt64', 13: 'string'}\n",
    "\n",
    "# Retrieve data\n",
    "statements_train_data = pd.read_csv('data/statements/train.tsv', sep='\\t', header=None, dtype=dtypes, names=column_names)\n",
    "statements_test_data = pd.read_csv('data/statements/test.tsv', sep='\\t', header=None, dtype=dtypes, names=column_names)\n",
    "statements_validate_data = pd.read_csv('data/statements/valid.tsv', sep='\\t', header=None, dtype=dtypes, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19e25f09-f3ac-4af4-9024-71f998fad80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---- CLEANING TEXT DATA ----\n",
    "The following steps will be taken to clean data\n",
    "- all non-alphabetical data will be removed\n",
    "- all text will be made lowercase\n",
    "- \n",
    "\"\"\"\n",
    "\n",
    "# Regex function to clean strings\n",
    "def regex_cleaner(text):\n",
    "    try:\n",
    "    # Make all text lowercase\n",
    "        text_lowercase = text.lower()\n",
    "    # Remove all non-alphanumeric text\n",
    "        text_alphanumeric = re.sub(r'[^a-z\\s\\-]', '', text_lowercase)\n",
    "    # Combine words that overlap to a new line\n",
    "        no_overlap = re.sub(r'(\\-\\n)', '', text_alphanumeric)\n",
    "    # remove \\n and \"-\"\n",
    "        no_new_line = re.sub(r'[\\n\\-]', ' ', no_overlap)\n",
    "    # Remove extra spacing\n",
    "        clean_text = re.sub(r'^\\s+', r' ', no_new_line)\n",
    "        return clean_text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Cleaning Statements Data\n",
    "statements_train_data['statement'] = statements_train_data['statement'].apply(regex_cleaner)\n",
    "statements_test_data['statement'] = statements_test_data['statement'].apply(regex_cleaner)\n",
    "statements_validate_data['statement'] = statements_validate_data['statement'].apply(regex_cleaner)\n",
    "\n",
    "# Clean labels\n",
    "label_dict = {'barely-true': 0, 'false': 1, 'half-true': 2, 'mostly-true': 3,\\\n",
    "              'pants-fire': 4, 'true': 5}\n",
    "label_changer = lambda ele: label_dict.get(ele)\n",
    "\n",
    "statements_train_data['label'] = statements_train_data['label'].apply(label_changer)\n",
    "statements_test_data['label'] = statements_test_data['label'].apply(label_changer)\n",
    "statements_validate_data['label'] = statements_validate_data['label'].apply(label_changer)\n",
    "\n",
    "\n",
    "# Cleaning Fake News Articles Data\n",
    "buzz_feed_fake_data = pd.read_csv('data/fake_news_articles/BuzzFeed_fake_news_content.csv')\n",
    "buzz_feed_real_data = pd.read_csv('data/fake_news_articles/BuzzFeed_real_news_content.csv')\n",
    "politi_fact_news_fake_data = pd.read_csv('data/fake_news_articles/PolitiFact_fake_news_content.csv')\n",
    "politi_fact_news_real_data = pd.read_csv('data/fake_news_articles/PolitiFact_real_news_content.csv')\n",
    "\n",
    "buzz_feed_fake_data['text'] = buzz_feed_fake_data['text'].apply(regex_cleaner)\n",
    "buzz_feed_real_data['text'] = buzz_feed_real_data['text'].apply(regex_cleaner)\n",
    "politi_fact_news_fake_data['text'] = politi_fact_news_fake_data['text'].apply(regex_cleaner)\n",
    "politi_fact_news_real_data['text'] = politi_fact_news_real_data['text'].apply(regex_cleaner)\n",
    "\n",
    "\n",
    "\n",
    "# Fake news two data cleaning\n",
    "fake_news = pd.read_csv('data/fake_news/WELFake_Dataset.csv')\n",
    "fake_news['text'] = fake_news['text'].apply(regex_cleaner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
