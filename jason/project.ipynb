{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28eb37c1-5f14-43b5-a219-f18fa0a4013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "# ---- LIBRARY IMPORTS ----\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ---- DEVICE SET UP ----\n",
    "def get_gpu():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"Using mps\")\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"Using cuda\")\n",
    "        return\"cuda\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "device = get_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb772a-86be-425e-bf89-40181898b342",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d6cd13-e73f-48a2-9cdb-cce9471542e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- DATA IMPORT ----\n",
    "# # Specify data types for each column\n",
    "# dtypes = {0: \"UInt64\", 1: \"string\", 2: \"string\", \"3\": \"UInt64\"}\n",
    "# # Import data\n",
    "# data_raw = pd.read_csv('data/fake_news/news_data.csv', dtype=dtypes)\n",
    "# # Remove NA values\n",
    "# data_no_na = data_raw.dropna()\n",
    "# # Drop columns \n",
    "# data_drop_columns = data_no_na.drop(columns=['Unnamed: 0', 'title'])\n",
    "# # Drop duplicate rows\n",
    "# clean_data = data_drop_columns.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e25f09-f3ac-4af4-9024-71f998fad80e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stop_words\u001b[39m(text):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words)\n\u001b[0;32m---> 15\u001b[0m clean_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mclean_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stop_words)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- LEMMATIZE THE TEXT ----\u001b[39;00m\n\u001b[1;32m     19\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---- CLEANING TEXT DATA ----\n",
    "The following steps will be taken to clean data\n",
    "- all non-alphabetical data will be removed\n",
    "- all emails and web urls will be removed\n",
    "- all stop words will be removed based on the stop words in the ntlk dictionary\n",
    "- all text will be made lowercase\n",
    "\"\"\"\n",
    "\n",
    "# ---- REMOVING STOP WORDS ----\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "clean_data['text'] = clean_data['text'].apply(remove_stop_words)\n",
    "\n",
    "\n",
    "# --- LEMMATIZE THE TEXT ----\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v' \n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n' \n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  \n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "# function to lemmatize words\n",
    "def pos_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Regex function to clean strings\n",
    "def regex_cleaner(text):\n",
    "    try:\n",
    "    # remove any web urls\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) \n",
    "    # remove email addresses\n",
    "        text = re.sub(r'\\b\\w+@\\w+\\.\\w+\\b', '', text) \n",
    "    # Make all text lowercase\n",
    "        text_lowercase = text.lower()\n",
    "    # Remove all non-alphanumeric text\n",
    "        text_alphanumeric = re.sub(r'[^a-z\\s\\-]', '', text_lowercase)\n",
    "    # Combine words that overlap to a new line\n",
    "        no_overlap = re.sub(r'(\\-\\n)', '', text_alphanumeric)\n",
    "    # remove \\n and \"-\"\n",
    "        no_new_line = re.sub(r'[\\n\\-]', ' ', no_overlap)\n",
    "    # Remove extra spacing\n",
    "        clean_text = re.sub(r'\\s+', r' ', no_new_line)\n",
    "        return clean_text\n",
    "    except:\n",
    "        raise Exception(\"Something went wrong :(\")\n",
    "\n",
    "# # Clean text data!\n",
    "# clean_data['text'] = clean_data['text'].apply(regex_cleaner)\n",
    "# clean_data['text'] = clean_data['text'].apply(lambda x: pos_lemmatize(str(x)))\n",
    "\n",
    "# # Drop NA values if any\n",
    "# clean_data = clean_data.dropna()\n",
    "\n",
    "# Write to csv to avoid wait time with tokenization\n",
    "# clean_data.to_csv('data/fake_news/clean_news_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad0426bb-f7e9-4279-9ae9-a471a490001f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- READ IN NEW DATA ----\n",
    "dypes = {'text': str, 'label': 'Int64'}\n",
    "clean_data = pd.read_csv(\"data/fake_news/clean_news_data.csv\", dtype=dypes)\n",
    "# Drop empty strings\n",
    "clean_data = clean_data.dropna()\n",
    "clean_data = clean_data.sample(n=1000, random_state=0)\n",
    "# clean_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44514224-7c1e-455f-99eb-340238498c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, test sets\n",
    "train, test = train_test_split(clean_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create an iterator object for train and test data\n",
    "data_iter = clean_data.iterrows()\n",
    "train_iter = train.iterrows()\n",
    "test_iter = test.iterrows()\n",
    "\n",
    "# Convert generators to list of tuples because DataLoader does not work well with pandas dataframes\n",
    "# Use this as inputs for DataLoader\n",
    "data_list_of_tuples = [(row.text, row.label) for index, row in data_iter]\n",
    "train_list_of_tuples = [(row.text, row.label) for index, row in train_iter]\n",
    "test_list_of_tuples = [(row.text, row.label) for index, row in test_iter]\n",
    "\n",
    "\"\"\"\n",
    "Tokenization for word sequences\n",
    "No tokenizer is required as data was tokenized in previous step.  We only require tokenizer to split articles by word to create the sequences\n",
    "Tokenizer = get_tokenizer(tokenizer=None)\n",
    "\"\"\"\n",
    "def yield_tokens(data):\n",
    "    \"\"\"\n",
    "    Pull the text data from series to tokenize it\n",
    "    Each row is a series when calling the iterrows() method, you must call the text column to pull its value\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer(tokenizer=None)\n",
    "    for index, row in data.iterrows():\n",
    "        text = row.text\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\"\"\"\n",
    "vocab_dict is now a function that takes a list of words as an input and returns integers based on the indexes found in the vocab_dict's dictionary\n",
    "<unk> -> In case a word is not in vocab_dict, we default it to a special index for words not in vocab_dict\n",
    "\"\"\"\n",
    "vocab_dict = build_vocab_from_iterator(iterator=yield_tokens(clean_data), specials=[\"<unk>\"])\n",
    "vocab_dict.set_default_index(vocab_dict[\"<unk>\"])\n",
    "# text_sequencer is a function that takes a string and returns a list of integers based off vocab_dict\n",
    "tokenizer = get_tokenizer(tokenizer=None)\n",
    "text_sequencer = lambda string: vocab_dict(tokenizer(string))\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch created from the DataLoader function and does data preprocessing to it\n",
    "    \"\"\"\n",
    "    labels, text_tensors_list = [], []\n",
    "    for example in batch:\n",
    "    # Get data from pandas series\n",
    "        text = example[0]\n",
    "        label = example[1]\n",
    "    # convert text to sequences of integers\n",
    "        text_sequence = text_sequencer(text)\n",
    "    # convert text_sequence to tensor\n",
    "        text_sequence_tensor = torch.tensor(text_sequence, dtype=torch.int64)\n",
    "    # append tensors to lists\n",
    "        labels.append(label)\n",
    "        text_tensors_list.append(text_sequence_tensor)\n",
    "# add padding of 0 to text_tensors (All articles have a different number of words and we want all tensors to be the same size)\n",
    "    text_tensors = pad_sequence(text_tensors_list, batch_first=True, padding_value = 0)\n",
    "# convert labels lists to tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "    return labels_tensor.to(device), text_tensors.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49242fb0-15af-4910-9dac-148c29fa199e",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239bb4ff-9d94-4ffa-ba06-77aee32a7346",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af27de0d-e868-4ca4-985b-d1d7cd5ed681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dimensions, nbr_layers_rnn, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    # this converts your sequence of words to a vector to pass through the NN\n",
    "        self.word_sequence_to_embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dimensions)\n",
    "        \"\"\"\n",
    "        Your RNN (bi-directional=True creates a bi-directional RNN)\n",
    "        - A bi-directional RNNs are well suited for jobs that call for knowledge of the context and connections among sequence elements\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(input_size=embedding_dimensions, hidden_size=hidden_size, num_layers=nbr_layers_rnn, batch_first=True,  dropout=0.2, bidirectional=False)\n",
    "    # Fully connected layer\n",
    "        self.fully_connected = nn.Linear(in_features=hidden_size, out_features=24)\n",
    "\n",
    "    # Apply xavier normal weights\n",
    "        torch.nn.init.xavier_normal_(self.fully_connected.weight)\n",
    "    # Drop out\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "    # fully_connected_two = nn.Linear\n",
    "        self.fully_connected_two = nn.Linear(in_features=24, out_features=1)\n",
    "    # Sigmoid activation\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "    # Create sequence embeddings\n",
    "        sequence_embeddings = self.word_sequence_to_embedding(input_)\n",
    "    # Compute output and hidden layers of RNN\n",
    "        output, hidden = self.rnn(sequence_embeddings, hidden)\n",
    "\n",
    "        \n",
    "    # Compute fully connected layer\n",
    "        fully_connected = self.fully_connected(output)\n",
    "\n",
    "    # Add drop out\n",
    "        drop = self.drop(fully_connected)\n",
    "\n",
    "        fully_connected_two = self.fully_connected_two(drop)\n",
    "        \n",
    "    # Since embeddings are of the shape (Example, term, term_features), we must reduce to a matrix by taking the average \n",
    "        fully_connected_reduced = fully_connected_two[:, -1, :]\n",
    "    # Sigmoid Activation\n",
    "        output = self.activation(fully_connected_reduced)\n",
    "        return output, hidden.detach()\n",
    "\n",
    "    def initHidden(self):\n",
    "    # Return a matrix of 1 row and k columns where k=hidden_size\n",
    "        return torch.zeros(nbr_layers_rnn, batch_size, self.hidden_size).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a97e8e-6d50-49d6-abf4-3b2221e86d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (word_sequence_to_embedding): Embedding(25683, 75)\n",
       "  (rnn): RNN(75, 128, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (fully_connected): Linear(in_features=128, out_features=24, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (fully_connected_two): Linear(in_features=24, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimensions = 75\n",
    "# this is the number of recurrent layers \n",
    "nbr_layers_rnn = 4\n",
    "n_hidden = 128\n",
    "batch_size = 4\n",
    "input_size = len(vocab_dict)\n",
    "rnn_model = RNN(input_size=input_size, hidden_size=n_hidden, embedding_dimensions=embedding_dimensions, nbr_layers_rnn=nbr_layers_rnn, batch_size=batch_size)\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed548a0-9400-414f-95e4-c5547aac794d",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ba5e39-fbbb-4e28-973e-cd51e31fddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sigmoid activations to labels\n",
    "def get_labels(results):\n",
    "    decision = lambda val: 1 if val >= 0.5 else 0\n",
    "    labels = torch.where(results > 0.5, 1.0, 0.0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c1d8028-66b2-4792-ac57-97f0ea54d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_list_of_tuples, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_list_of_tuples, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, drop_last=True)\n",
    "\n",
    "# Create Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create Optimizer\n",
    "# optimizer = optim.SGD(rnn_model.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.005)\n",
    "# Create Method to change learning rate by a factor of lr*0.1 after every epoch that doesn't have an effect on the loss function\n",
    "# scheduler = StepLR(optimizer=optimizer, step_size=10, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783c8959-122b-43aa-9120-0efb2528de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iter = 20\n",
    "def fit_rnn(model, trainloader, testloader, nbr_of_epochs, optimizer, criterion):\n",
    "# Dictionary of epoch results\n",
    "    epoch_results = {}\n",
    "# Iterate through epochs\n",
    "    for epoch in range(nbr_of_epochs):\n",
    "    # ---- MODEL TRAINING ----\n",
    "\n",
    "    # Initialize hidden layer\n",
    "        hidden = model.initHidden()\n",
    "    # Put model in training mode    \n",
    "        model.train()\n",
    "    # Start timer for trainig of epoch\n",
    "        start_time = time.perf_counter()\n",
    "    # Initalize the batch number currently being worked on\n",
    "        batch_nbr = 0\n",
    "    # Create loss variable for epoch\n",
    "        training_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        training_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        training_total_examples = 0.0\n",
    "    # Create loss variable for epoch\n",
    "        testing_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        testing_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        testing_total_examples = 0.0\n",
    "    # Execute Forward, Backward, Optimization\n",
    "    \n",
    "    # Create loss variable for epoch\n",
    "        testing_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        testing_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        testing_total_examples = 0.0\n",
    "\n",
    "\n",
    "    # Iterate through data\n",
    "        for labels, inputs in trainloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        # Detach the hidden layer so we don't compute gradient across hidden layers\n",
    "            hidden = hidden.detach() \n",
    "        # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # ---- FORWARD, BACKWARD, OPTIMIZE ----\n",
    "        # Get model training predictions\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = torch.squeeze(outputs, 1)\n",
    "        # Convert model training predictions to their respective classifications\n",
    "            predicted_labels = get_labels(outputs)\n",
    "        # Compute Loss of current batch            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # ---- LASSO Regularization ----\n",
    "        # Initialize regularization loss penalty\n",
    "            total_abs_weight = sum([torch.sum(torch.abs(beta)).item() for beta in model.parameters()])\n",
    "            total_number_of_model_weights = sum([torch.numel(beta) for beta in model.parameters()])\n",
    "            avg_abs_weight = total_abs_weight/total_number_of_model_weights\n",
    "        # Add penaltiy to loss\n",
    "            lambda_ = 0.01\n",
    "            loss += lambda_ * avg_abs_weight\n",
    "\n",
    "        # Compute total number of correctly classified examples\n",
    "            nbr_of_correct_predictions = torch.sum(predicted_labels == labels).item()\n",
    "        # Grab batch size\n",
    "            total_nbr_of_elements = labels.shape[0]\n",
    "        # Compute number of correctly labeled examples and the total exampes\n",
    "            training_total_correct += nbr_of_correct_predictions\n",
    "            training_total_examples += total_nbr_of_elements\n",
    "\n",
    "        # del labels, inputs from memory\n",
    "            del labels\n",
    "            del outputs\n",
    "            \n",
    "        # Backward pass\n",
    "            loss.backward() \n",
    "        # Update model params with gradient clippings\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.5)\n",
    "        # Initalize the next step of optimizer\n",
    "            optimizer.step()\n",
    "        # update training loss of epoch\n",
    "            training_losses += float(loss.item())*total_nbr_of_elements\n",
    "        #  update the current batch number of epoch\n",
    "            batch_nbr += 1\n",
    "    # End training time\n",
    "        end_time = time.perf_counter()\n",
    "    # Get total runntime of epoch\n",
    "        epoch_runtime = timedelta(seconds=end_time-start_time).total_seconds()\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # init hidden state\n",
    "            hidden_test = model.initHidden()\n",
    "    \n",
    "        # Iterate through data\n",
    "            for labels, inputs in testloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            # Detach the hidden layer so we don't compute gradient across hidden layers\n",
    "                hidden_test = hidden_test.detach() \n",
    "            \n",
    "                \n",
    "            # ---- FORWARD, BACKWARD, OPTIMIZE ----\n",
    "            # Get model training predictions\n",
    "                outputs, hidden_test = model(inputs, hidden_test)\n",
    "                outputs = torch.squeeze(outputs, 1)\n",
    "            # Convert model training predictions to their respective classifications\n",
    "                predicted_labels = get_labels(outputs)\n",
    "            # Compute Loss of current batch            \n",
    "                loss = criterion(outputs, labels)\n",
    "    \n",
    "            # ---- LASSO Regularization ----\n",
    "            # Initialize regularization loss penalty\n",
    "                total_abs_weight = sum([torch.sum(torch.abs(beta)).item() for beta in model.parameters()])\n",
    "                total_number_of_model_weights = sum([torch.numel(beta) for beta in model.parameters()])\n",
    "                avg_abs_weight = total_abs_weight/total_number_of_model_weights\n",
    "            # Add penaltiy to loss\n",
    "                lambda_ = 0.01\n",
    "                loss += lambda_ * avg_abs_weight\n",
    "    \n",
    "            # Compute total number of correctly classified examples\n",
    "                nbr_of_correct_predictions = torch.sum(predicted_labels == labels).item()\n",
    "            # Grab batch size\n",
    "                total_nbr_of_elements = labels.shape[0]\n",
    "            # Compute number of correctly labeled examples and the total exampes\n",
    "                testing_total_correct += nbr_of_correct_predictions\n",
    "                testing_total_examples += total_nbr_of_elements\n",
    "    \n",
    "    \n",
    "        # ---- COMPUTING ACCURACY/LOSS ----\n",
    "        # Compute training accuracy/loss of epoch\n",
    "            total_training_epoch_loss = round(training_losses/len(trainloader), 4)\n",
    "            training_accuracy = round(training_total_correct/training_total_examples, 4)\n",
    "        # Compute training accuracy/loss of epoch\n",
    "            total_testing_epoch_loss = round(testing_losses/len(testloader), 4)\n",
    "            testing_accuracy = round(testing_total_correct/testing_total_examples, 4)\n",
    "    \n",
    "        # ---- LEARNING RATE ----\n",
    "        # # Current learning rate\n",
    "        #     current_lr = scheduler.get_last_lr()\n",
    "        # # Adjust learning rate\n",
    "        #     scheduler.step()\n",
    "    \n",
    "        # ---- RESULTS ----\n",
    "        # print results of epoch\n",
    "            print(f'Epoch {epoch + 1}/{epoch_iter} <-> Runtime: {round(epoch_runtime, 0)}s  <-> Training loss: {total_training_epoch_loss} <-> Training Accuracy: {training_accuracy} <-> Testing Loss: {total_training_epoch_loss} <-> Testing Accuracy: {testing_accuracy}')\n",
    "            # print(f'Epoch {epoch + 1}/{epoch_iter} <-> Runtime: {round(epoch_runtime, 0)}s <-> Learning Rate: {current_lr} <-> Training loss: {total_training_epoch_loss} <-> Training Accuracy: {training_accuracy} <-> Testing Loss: {total_training_epoch_loss} <-> Testing Accuracy: {testing_accuracy}')\n",
    "        # Update results of epochs\n",
    "            result_dict = { \"runtime\": epoch_runtime,#\"learning_rate\": current_lr, \n",
    "                           \"training_loss\": total_training_epoch_loss, 'training_accuracy': training_accuracy,\n",
    "                           \"testing_loss\": total_testing_epoch_loss, 'testing_accuracy': testing_accuracy,}\n",
    "            epoch_results.update({epoch+1: result_dict})\n",
    "    return epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ab423f-25b3-4d5b-a301-68f8fef6ae80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 <-> Runtime: 210.0s  <-> Training loss: 3.0339 <-> Training Accuracy: 0.5487 <-> Testing Loss: 3.0339 <-> Testing Accuracy: 0.52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m epochs_results \u001b[38;5;241m=\u001b[39m \u001b[43mfit_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbr_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 79\u001b[0m, in \u001b[0;36mfit_rnn\u001b[0;34m(model, trainloader, testloader, nbr_of_epochs, optimizer, criterion)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m outputs\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Update model params with gradient clippings\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2.5\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_results = fit_rnn(model=rnn_model, trainloader=train_loader, testloader=test_loader, nbr_of_epochs=epoch_iter, optimizer=optimizer, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db1331-0d4c-44b5-b6e1-aad315a202de",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3e068-fb57-48c3-9120-a08e210012fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOTTING GRAPH OF LSS AND ACCURACY TO NUMBER OF EPOCH ----\n",
    "fig, ((ax_one), (ax_two)) = plt.subplots(1, 2)\n",
    "\n",
    "\n",
    "# ---- TRAINING AND TESTING ACCURACY TO EPOCH ----\n",
    "epoch_accuracy = {\"training_accuracy\": [epochs_results.get(epoch).get('training_accuracy') for epoch in range(1, max(epochs_results.keys())+1)], \"testing_accuracy\": [epochs_results.get(epoch).get('testing_accuracy') for epoch in range(1, max(epochs_results.keys())+1)], \"epoch\":[epoch for epoch in range(1, max(epochs_results.keys())+1)]}\n",
    "ax_one.plot(epoch_accuracy.get('epoch'), epoch_accuracy.get('training_accuracy'), linestyle='-', marker='o')\n",
    "ax_one.plot(epoch_accuracy.get('epoch'), epoch_accuracy.get('testing_accuracy'), linestyle='-', marker='o')\n",
    "ax_one.legend(['Training Accuracy', 'Testing Accuracy'])\n",
    "ax_one.set_xticks(epoch_accuracy.get('epoch'), labels=epoch_accuracy.get('epoch'))\n",
    "ax_one.set_xlabel(\"Epoch\")\n",
    "ax_one.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# ---- TRAINING AND TESTING LOSS TO EPOCH ----\n",
    "epoch_loss = {\"training_loss\": [epochs_results.get(epoch).get('training_loss') for epoch in range(1, max(epochs_results.keys())+1)], \"testing_loss\": [epochs_results.get(epoch).get('testing_loss') for epoch in range(1, max(epochs_results.keys())+1)], \"epoch\":[epoch for epoch in range(1, max(epochs_results.keys())+1)]}\n",
    "ax_two.plot(epoch_loss.get('epoch'), epoch_loss.get('training_loss'), linestyle='-', marker='o')\n",
    "ax_two.plot(epoch_loss.get('epoch'), epoch_loss.get('testing_loss'), linestyle='-', marker='o')\n",
    "ax_two.legend(['Training Loss', 'Testing Loss'])\n",
    "ax_two.set_xticks(epoch_loss.get('epoch'), labels=epoch_loss.get('epoch'))\n",
    "ax_two.set_xlabel(\"Epoch\")\n",
    "ax_two.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21273027-e634-4885-9c8f-0905d4ccda6b",
   "metadata": {},
   "source": [
    "# References\n",
    "[https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "[https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)\n",
    "\n",
    "[https://www.geeksforgeeks.org/adjusting-learning-rate-of-a-neural-network-in-pytorch/#](https://www.geeksforgeeks.org/adjusting-learning-rate-of-a-neural-network-in-pytorch/#)\n",
    "\n",
    "[https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "[Bi-directional RNN](https://www.geeksforgeeks.org/bidirectional-recurrent-neural-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e51bd7-a0e8-454f-b169-caefee6da3bb",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78f28329-b8ef-4c19-acf6-532bf419a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dimensions, nbr_layers_rnn, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    # this converts your sequence of words to a vector to pass through the NN\n",
    "        self.word_sequence_to_embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dimensions)\n",
    "        \"\"\"\n",
    "        Your RNN (bi-directional=True creates a bi-directional RNN)\n",
    "        - A bi-directional RNNs are well suited for jobs that call for knowledge of the context and connections among sequence elements\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dimensions, hidden_size=hidden_size, num_layers=nbr_layers_rnn, batch_first=True,  dropout=0.2, bidirectional=False)\n",
    "    # Fully connected layer\n",
    "        self.fully_connected = nn.Linear(in_features=hidden_size, out_features=24)\n",
    "\n",
    "    # Apply xavier normal weights\n",
    "        torch.nn.init.xavier_normal_(self.fully_connected.weight)\n",
    "    # Drop out\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "    # fully_connected_two = nn.Linear\n",
    "        self.fully_connected_two = nn.Linear(in_features=24, out_features=1)\n",
    "    # Sigmoid activation\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_, hidden_tuple):\n",
    "    # Create sequence embeddings\n",
    "        sequence_embeddings = self.word_sequence_to_embedding(input_)\n",
    "    # Compute output and hidden layers of RNN\n",
    "        output, hidden_tuple = self.lstm(sequence_embeddings, hidden_tuple)\n",
    "\n",
    "        \n",
    "    # Compute fully connected layer\n",
    "        fully_connected = self.fully_connected(output)\n",
    "\n",
    "    # Add drop out\n",
    "        drop = self.drop(fully_connected)\n",
    "\n",
    "        fully_connected_two = self.fully_connected_two(drop)\n",
    "        \n",
    "    # Since embeddings are of the shape (Example, term, term_features), we must reduce to a matrix by taking the average \n",
    "        fully_connected_reduced = fully_connected_two[:, -1, :]\n",
    "    # Sigmoid Activation\n",
    "        output = self.activation(fully_connected_reduced)\n",
    "        return output, hidden_tuple\n",
    "\n",
    "    def initHidden(self):\n",
    "    # Return a matrix of 1 row and k columns where k=hidden_size\n",
    "        return (torch.zeros(nbr_layers_rnn, batch_size, self.hidden_size).to(device), torch.zeros(nbr_layers_rnn, batch_size, self.hidden_size).to(device))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b74e044d-5477-4e2a-90f4-a3a5adeebb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (word_sequence_to_embedding): Embedding(5037, 75)\n",
       "  (lstm): LSTM(75, 128, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (fully_connected): Linear(in_features=128, out_features=24, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (fully_connected_two): Linear(in_features=24, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimensions = 75\n",
    "# this is the number of recurrent layers \n",
    "nbr_layers_rnn = 4\n",
    "n_hidden = 128\n",
    "batch_size = 4\n",
    "input_size = len(vocab_dict)\n",
    "lstm_model = LSTM(input_size=input_size, hidden_size=n_hidden, embedding_dimensions=embedding_dimensions, nbr_layers_rnn=nbr_layers_rnn, batch_size=batch_size)\n",
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5d34a72-86e7-41bb-9c4d-dcc5d255d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss function\n",
    "lstm_criterion = nn.BCELoss()\n",
    "\n",
    "# Create Optimizer\n",
    "lstm_optimizer = optim.SGD(lstm_model.parameters(), lr=0.005, momentum=0.9)\n",
    "# lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.005)\n",
    "# Create Method to change learning rate by a factor of lr*0.1 after every epoch that doesn't have an effect on the loss function\n",
    "# scheduler = StepLR(optimizer=optimizer, step_size=2, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d0f2aa4-526e-4dce-92cf-6e6a58545959",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iter = 2\n",
    "def fit_lstm(model, trainloader, testloader, nbr_of_epochs, optimizer, criterion):\n",
    "# Dictionary of epoch results\n",
    "    epoch_results = {}\n",
    "# Put model in training mode    \n",
    "    model.train()\n",
    "# Iterate through epochs\n",
    "    for epoch in range(nbr_of_epochs):\n",
    "    # ---- MODEL TRAINING ----\n",
    "\n",
    "    # Initialize hidden layer\n",
    "        hidden_tuple = model.initHidden()\n",
    "    \n",
    "    # Start timer for trainig of epoch\n",
    "        start_time = time.perf_counter()\n",
    "    # Initalize the batch number currently being worked on\n",
    "        batch_nbr = 0\n",
    "    # Create loss variable for epoch\n",
    "        training_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        training_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        training_total_examples = 0.0\n",
    "    # Create loss variable for epoch\n",
    "        testing_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        testing_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        testing_total_examples = 0.0\n",
    "    # Execute Forward, Backward, Optimization\n",
    "    \n",
    "    # Create loss variable for epoch\n",
    "        testing_losses = 0.0\n",
    "    # Total examples labeld correctly in epoch\n",
    "        testing_total_correct = 0.0\n",
    "    # Total number of examples in epoch\n",
    "        testing_total_examples = 0.0\n",
    "\n",
    "        \n",
    "    # Iterate through data\n",
    "        for labels, inputs in trainloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        # Detach the hidden layer so we don't compute gradient across hidden layers\n",
    "            # hidden_tuple[0].detach()\n",
    "            # hidden_tuple[1].detach()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ---- FORWARD, BACKWARD, OPTIMIZE ----\n",
    "        # Get model training predictions\n",
    "            outputs, hidden_tuple = model(inputs, hidden_tuple)\n",
    "            outputs = torch.squeeze(outputs, 1)\n",
    "        # Convert model training predictions to their respective classifications\n",
    "            predicted_labels = get_labels(outputs)\n",
    "        # Compute Loss of current batch            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # ---- LASSO Regularization ----\n",
    "        # Initialize regularization loss penalty\n",
    "            total_abs_weight = sum([torch.sum(torch.abs(beta)).item() for beta in model.parameters()])\n",
    "            total_number_of_model_weights = sum([torch.numel(beta) for beta in model.parameters()])\n",
    "            avg_abs_weight = total_abs_weight/total_number_of_model_weights\n",
    "        # Add penaltiy to loss\n",
    "            lambda_ = 0.01\n",
    "            loss += lambda_ * avg_abs_weight\n",
    "\n",
    "        # Compute total number of correctly classified examples\n",
    "            nbr_of_correct_predictions = torch.sum(predicted_labels == labels).item()\n",
    "        # Grab batch size\n",
    "            total_nbr_of_elements = labels.shape[0]\n",
    "        # Compute number of correctly labeled examples and the total exampes\n",
    "            training_total_correct += nbr_of_correct_predictions\n",
    "            training_total_examples += total_nbr_of_elements\n",
    "\n",
    "        # Backward pass\n",
    "            loss.backward(retain_graph=True) \n",
    "        # Update model params with gradient clippings\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.5)\n",
    "        # Initalize the next step of optimizer\n",
    "            optimizer.step()\n",
    "        # update training loss of epoch\n",
    "            training_losses += float(loss.item())*total_nbr_of_elements\n",
    "        #  update the current batch number of epoch\n",
    "            batch_nbr += 1\n",
    "    # End training time\n",
    "        end_time = time.perf_counter()\n",
    "    # Get total runntime of epoch\n",
    "        epoch_runtime = timedelta(seconds=end_time-start_time).total_seconds()\n",
    "\n",
    "\n",
    "    # init hidden state\n",
    "        hidden_test_tuple = model.initHidden()\n",
    "\n",
    "    # Iterate through data\n",
    "        for labels, inputs in testloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        # Detach the hidden layer so we don't compute gradient across hidden layers\n",
    "            hidden_test_tuple[0].detach()\n",
    "            hidden_test_tuple[1].detach()\n",
    "            # hidden_test_tuple = (hidden_test_tuple[0].detach, hidden_test_tuple[1].detach)\n",
    "        \n",
    "            \n",
    "        # ---- FORWARD, BACKWARD, OPTIMIZE ----\n",
    "        # Get model training predictions\n",
    "            outputs, hidden_test_tuple = model(inputs, hidden_test_tuple)\n",
    "            outputs = torch.squeeze(outputs, 1)\n",
    "        # Convert model training predictions to their respective classifications\n",
    "            predicted_labels = get_labels(outputs)\n",
    "        # Compute Loss of current batch            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # ---- LASSO Regularization ----\n",
    "        # Initialize regularization loss penalty\n",
    "            total_abs_weight = sum([torch.sum(torch.abs(beta)).item() for beta in model.parameters()])\n",
    "            total_number_of_model_weights = sum([torch.numel(beta) for beta in model.parameters()])\n",
    "            avg_abs_weight = total_abs_weight/total_number_of_model_weights\n",
    "        # Add penaltiy to loss\n",
    "            lambda_ = 0.01\n",
    "            loss += lambda_ * avg_abs_weight\n",
    "\n",
    "        # Compute total number of correctly classified examples\n",
    "            nbr_of_correct_predictions = torch.sum(predicted_labels == labels).item()\n",
    "        # Grab batch size\n",
    "            total_nbr_of_elements = labels.shape[0]\n",
    "        # Compute number of correctly labeled examples and the total exampes\n",
    "            testing_total_correct += nbr_of_correct_predictions\n",
    "            testing_total_examples += total_nbr_of_elements\n",
    "\n",
    "        # del labels, inputs from memory\n",
    "            del labels\n",
    "            del outputs\n",
    "\n",
    "    # ---- COMPUTING ACCURACY/LOSS ----\n",
    "    # Compute training accuracy/loss of epoch\n",
    "        total_training_epoch_loss = round(training_losses/len(trainloader), 4)\n",
    "        training_accuracy = round(training_total_correct/training_total_examples, 4)\n",
    "    # Compute training accuracy/loss of epoch\n",
    "        total_testing_epoch_loss = round(testing_losses/len(testloader), 4)\n",
    "        testing_accuracy = round(testing_total_correct/testing_total_examples, 4)\n",
    "\n",
    "    # ---- LEARNING RATE ----\n",
    "    # Current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "    # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # ---- RESULTS ----\n",
    "    # print results of epoch\n",
    "        # print(f'Epoch {epoch + 1}/{epoch_iter} <-> Runtime: {round(epoch_runtime, 0)}s <-> Training loss: {total_training_epoch_loss} <-> Training Accuracy: {training_accuracy} <-> Testing Loss: {total_training_epoch_loss} <-> Testing Accuracy: {testing_accuracy}')\n",
    "        print(f'Epoch {epoch + 1}/{epoch_iter} <-> Runtime: {round(epoch_runtime, 0)}s <-> Learning Rate: {current_lr} <-> Training loss: {total_training_epoch_loss} <-> Training Accuracy: {training_accuracy} <-> Testing Loss: {total_training_epoch_loss} <-> Testing Accuracy: {testing_accuracy}')\n",
    "    # Update results of epochs\n",
    "        # result_dict = { \"runtime\": epoch_runtime,\n",
    "        #                \"training_loss\": total_training_epoch_loss, 'training_accuracy': training_accuracy,\n",
    "        #                \"testing_loss\": total_testing_epoch_loss, 'testing_accuracy': testing_accuracy,}\n",
    "        result_dict = { \"runtime\": epoch_runtime, #\"learning_rate\": current_lr, \n",
    "                       \"training_loss\": total_training_epoch_loss, 'training_accuracy': training_accuracy,\n",
    "                       \"testing_loss\": total_testing_epoch_loss, 'testing_accuracy': testing_accuracy,}\n",
    "        epoch_results.update({epoch+1: result_dict})\n",
    "    return epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2278e669-6adb-464c-9468-99e4f60b371d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [512, 75]] is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m epochs_results \u001b[38;5;241m=\u001b[39m \u001b[43mfit_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbr_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_criterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 78\u001b[0m, in \u001b[0;36mfit_lstm\u001b[0;34m(model, trainloader, testloader, nbr_of_epochs, optimizer, criterion)\u001b[0m\n\u001b[1;32m     75\u001b[0m     training_total_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_nbr_of_elements\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Update model params with gradient clippings\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2.5\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [512, 75]] is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "epochs_results = fit_lstm(model=lstm_model, trainloader=train_loader, testloader=test_loader, nbr_of_epochs=epoch_iter, optimizer=lstm_optimizer, criterion=lstm_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8302d-fe1f-4615-b145-bb31d950b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOTTING GRAPH OF LSS AND ACCURACY TO NUMBER OF EPOCH ----\n",
    "fig, ((ax_one), (ax_two)) = plt.subplots(1, 2)\n",
    "\n",
    "\n",
    "# ---- TRAINING AND TESTING ACCURACY TO EPOCH ----\n",
    "epoch_accuracy = {\"training_accuracy\": [epochs_results.get(epoch).get('training_accuracy') for epoch in range(1, max(epochs_results.keys())+1)], \"testing_accuracy\": [epochs_results.get(epoch).get('testing_accuracy') for epoch in range(1, max(epochs_results.keys())+1)], \"epoch\":[epoch for epoch in range(1, max(epochs_results.keys())+1)]}\n",
    "ax_one.plot(epoch_accuracy.get('epoch'), epoch_accuracy.get('training_accuracy'), linestyle='-', marker='o')\n",
    "ax_one.plot(epoch_accuracy.get('epoch'), epoch_accuracy.get('testing_accuracy'), linestyle='-', marker='o')\n",
    "ax_one.legend(['Training Accuracy', 'Testing Accuracy'])\n",
    "ax_one.set_xticks(epoch_accuracy.get('epoch'), labels=epoch_accuracy.get('epoch'))\n",
    "ax_one.set_xlabel(\"Epoch\")\n",
    "ax_one.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# ---- TRAINING AND TESTING LOSS TO EPOCH ----\n",
    "epoch_loss = {\"training_loss\": [epochs_results.get(epoch).get('training_loss') for epoch in range(1, max(epochs_results.keys())+1)], \"testing_loss\": [epochs_results.get(epoch).get('testing_loss') for epoch in range(1, max(epochs_results.keys())+1)], \"epoch\":[epoch for epoch in range(1, max(epochs_results.keys())+1)]}\n",
    "ax_two.plot(epoch_loss.get('epoch'), epoch_loss.get('training_loss'), linestyle='-', marker='o')\n",
    "ax_two.plot(epoch_loss.get('epoch'), epoch_loss.get('testing_loss'), linestyle='-', marker='o')\n",
    "ax_two.legend(['Training Loss', 'Testing Loss'])\n",
    "ax_two.set_xticks(epoch_loss.get('epoch'), labels=epoch_loss.get('epoch'))\n",
    "ax_two.set_xlabel(\"Epoch\")\n",
    "ax_two.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
