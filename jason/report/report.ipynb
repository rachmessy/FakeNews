{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809e7a17-b888-4398-9182-f5193d7b3cc7",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Technology today has improve exponentially, and the world wide web is at the heart of it.  It connects billions of people across the globe, providing them with countless forms of entertainment.  Since the internet is not very well regulated, people have the freedom to post whatever they want, regardless if what they post is factual or fake.  The aim of this project is to create a deep learning model that can determine if an article is a real or fake.\n",
    "\n",
    "We have decided to implement two models.  The first is a multiple layered Recurrent Neural Network, with its first implemented layer being a third order tensor where each matrix in the third order is a matrix of word vectors.  The next layer implemented is the RNN, followed by two fully connected layers, and a sigmoid activation function for binary classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336c2a2-f51d-458f-9b22-416c183fbde4",
   "metadata": {},
   "source": [
    "# Section 1 - The Dataset\n",
    "\n",
    "We have used the [WELFake](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification) dataset that contains a well balanced mix of real and fake news articles.  This dataset is a combination of datasets from Kaggle, Mclintire, Reuters, and BuzzFeed.  There are a total of 78098 examples provided in this dataset with four features\n",
    "\n",
    "1. Serial Number\n",
    "2. Title\n",
    "3. Text\n",
    "4. Label\n",
    "\n",
    "Since we only care about the article itself and its label, we will focus our data preprocessing in those two features.\n",
    "\n",
    "## Data Preprocessing\n",
    "Starting with the raw dataset, we want to remove anything in the articles that looked like emails and web urls.  We then removed all non-alphanumeric characters and lowercased them.  Finally, all words were lemmatized and stop words were removed.  These data preprocessing steps were done with regular expressions. Following the preprocessing of the raw data, the text was required to be tokenized and converted to integers.  Therefore a dictionary was created where each token was tied to an integer.  Each text was then converted to a sequenece of integers.  Since some text were larger than others, smaller text was padded with zeros.  After all text was converted to integers, the sequences were converted into word vectors with a length of the largest article to be used in the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150bc15-9015-4701-936e-dc0912ce7cb6",
   "metadata": {},
   "source": [
    "# Section 2 - Model Descriptions\n",
    "We have tested a regular [Recurrent Neural Network](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), then a Neural Network with [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). The reason we decided to choose these two models is because both of these models can capture trends in sequential data.\n",
    "\n",
    "## Recurrent Neural Network\n",
    "We have applied a RNN using a `tanh` activation for the output of the hidden layer.  The RNN had\n",
    "\n",
    "- 4 stacked recurrent layers\n",
    "- A design matrix of 100 features\n",
    "- 128 features for each hidden state\n",
    "- A drop out layer with drop out probability of $p= 0.5$\n",
    "\n",
    "The data being supplied as an input to the RNN is the output of a word embeddings layer with an output of 100 features.  Following the RNN layers, we have a fully connected layer with an output of 64 features, another drop out layer with drop out probability of $p= 0.5$, and another fully connected layer with one output.  The output is supplied to a sigmoid activation function for binary classification. We have also applied an xavier normally distribution for weights for the first fully connected layer following the RNN layers and a gradient clipping with a clipping threshold of $\\theta = 3$.\n",
    "\n",
    "As Both models ran into memory issues, we settled with the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14440567-6fe6-4b1e-9660-4363e2b90876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dimensions, nbr_layers_rnn, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_sequence_to_embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dimensions) # this converts your sequence of words to a vector to pass through the NN\n",
    "        self.rnn = nn.RNN(input_size=embedding_dimensions, hidden_size=hidden_size, num_layers=nbr_layers_rnn, batch_first=True,  dropout=0.5, bidirectional=False)\n",
    "        self.fully_connected = nn.Linear(in_features=hidden_size, out_features=64) # Fully connected layer\n",
    "        torch.nn.init.xavier_normal_(self.fully_connected.weight) # Apply xavier normal weights\n",
    "        self.drop = nn.Dropout(0.5)  # Drop out\n",
    "        self.fully_connected_two = nn.Linear(in_features=64, out_features=1)  # fully_connected_two = nn.Linear\n",
    "        self.activation = nn.Sigmoid() # Sigmoid activation\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        sequence_embeddings = self.word_sequence_to_embedding(input_)# Create sequence embeddings\n",
    "        output, hidden = self.rnn(sequence_embeddings, hidden) # Compute output and hidden layers of RNN\n",
    "        fully_connected = self.fully_connected(output) # Compute fully connected layer\n",
    "        drop = self.drop(fully_connected) # Add drop out\n",
    "        fully_connected_two = self.fully_connected_two(drop) # Second fully connected layer\n",
    "        fully_connected_reduced = fully_connected_two[:, -1, :] # Since embeddings are of the shape (Example, term, term_features), we must reduce to a matrix by taking the average \n",
    "        output = self.activation(fully_connected_reduced)     # Sigmoid Activation\n",
    "        return output, hidden.detach()\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(nbr_layers_rnn, batch_size, self.hidden_size).to(device)     # Return a matrix of 1 row and k columns where k=hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce9d85-4a87-46af-b5f0-97c1c298c927",
   "metadata": {},
   "source": [
    "# Section 3 - Loss Function\n",
    "We decided to use a [binary cross entropy loss function](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) since this was a binary classification task.  We also made sure to use an average reduction to reduce our output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c35dc-feed-4ca0-a636-af823d8fbe39",
   "metadata": {},
   "source": [
    "# Section 4 - Optimization\n",
    "We have decided to use the Stochastic Gradient Descent Optimization function with a learning rate of $r = 0.005$.  We have experimented with a variety of learning rates, ranging from 0.001, to 0.05.  We also applied a [step learning rate](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR) where the learning reduced by a factor of 0.1 every 4 epochs.  Overall the the constant learning rate of $r=0.005$ appeared to provide a more constistent training and testing accuracy.  We also tried applying the [ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau) function that allowed us to reduce the learning rate by a factor of 0.1 every time we had two consecutive epochs containing about the same total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81fa5a-ace8-4930-a7e4-14edb8811c5e",
   "metadata": {},
   "source": [
    "# Section 5 - Metrics and Experimental Results\n",
    "## RNN\n",
    "\n",
    "<br>\n",
    "\n",
    "![](rnn_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a346fdb-e9a0-4db2-a90f-ddaa95656bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rachel Messenger"
   },
   {
    "name": "Jason Caballes"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "title": "Group 13"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
